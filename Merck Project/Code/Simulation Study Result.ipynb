{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the results\n",
    "# Default value\n",
    "def_value_result = pd.read_csv(\"def_value_result.csv\")\n",
    "# Tune One - Random Forest\n",
    "rf_n_estimators = pd.read_csv(\"rf_n_estimators.csv\")\n",
    "rf_bootstrap = pd.read_csv(\"rf_bootstrap.csv\")\n",
    "rf_max_samples = pd.read_csv(\"rf_max_samples.csv\")\n",
    "rf_max_features = pd.read_csv(\"rf_max_features.csv\")\n",
    "rf_min_samples_leaf = pd.read_csv(\"rf_min_samples_leaf.csv\")\n",
    "# Tune One - XGBoost\n",
    "xgb_num_boost_round = pd.read_csv(\"xgb_num_boost_round.csv\")\n",
    "xgb_lambda = pd.read_csv(\"xgb_lambda.csv\")\n",
    "xgb_alpha = pd.read_csv(\"xgb_alpha.csv\")\n",
    "xgb_eta = pd.read_csv(\"xgb_eta.csv\")\n",
    "xgb_subsample = pd.read_csv(\"xgb_subsample.csv\")\n",
    "xgb_max_depth = pd.read_csv(\"xgb_max_depth.csv\")\n",
    "xgb_min_child_weight = pd.read_csv(\"xgb_min_child_weight.csv\")\n",
    "xgb_colsample_bytree = pd.read_csv(\"xgb_colsample_bytree.csv\")\n",
    "xgb_colsample_bylevel = pd.read_csv(\"xgb_colsample_bylevel.csv\")\n",
    "# Grid Search\n",
    "rf_grid_result = pd.read_csv(\"rf_grid_result.csv\")\n",
    "xgb_grid_result = pd.read_csv(\"xgb_grid_result.csv\")\n",
    "# Random Search\n",
    "rf_rs_result = pd.read_csv(\"rf_rs_result.csv\")\n",
    "xgb_rs_result = pd.read_csv(\"xgb_rs_result.csv\")\n",
    "# Bayesian Optimization\n",
    "rf_bo_result = pd.read_csv(\"rf_bo_result.csv\")\n",
    "xgb_bo_result = pd.read_csv(\"xgb_bo_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the datasets\n",
    "df_list = []\n",
    "dmat_list = []\n",
    "num_datasets = 27\n",
    "for i in range(num_datasets):\n",
    "    df = pd.read_csv(\"df{}.csv\".format(i)) \n",
    "    df_list.append(df)\n",
    "    dmat = xgb.DMatrix(df.iloc[:,1:df.shape[1]], label=df.iloc[:,0])\n",
    "    dmat_list.append(dmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "# Define the parameter search space\n",
    "dval = {\n",
    "    'n_estimators': 100, # \"num.trees\": 500 / \"n_estimators\": 100\n",
    "    'criterion': 'mse',\n",
    "    'bootstrap': True, # replace\n",
    "    'max_samples': None, # sample.fraction\n",
    "    'max_features': 'sqrt', ### \"mtry\": \"sqrt\" / \"max_features\": \"auto\" ###\n",
    "    'min_samples_leaf': 1, # \"min.node.size\" default: 5 / \"min_samples_leaf\": 1\n",
    "}\n",
    "# Fit the model to each dataset\n",
    "def_result = pd.DataFrame(columns=['rf_rmse'])\n",
    "mlist = []\n",
    "mod = RandomForestRegressor(**dval)\n",
    "for i in range(num_datasets):\n",
    "    num_col = df_list[i].shape[1]\n",
    "    yvec = df_list[i].iloc[:,0:1].values.ravel()\n",
    "    rmse = cross_val_score(mod, df_list[i].iloc[:,1:num_col], yvec, \n",
    "                           scoring=\"neg_root_mean_squared_error\", cv=5)\n",
    "    mlist.append(abs(rmse).mean())\n",
    "def_result['rf_rmse'] = mlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "# Define the parameter search space\n",
    "dval = {\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"lambda\": 1,\n",
    "    \"alpha\": 0,\n",
    "    \"eta\": 0.3,\n",
    "    \"subsample\": 1,\n",
    "    \"max_depth\": 6,\n",
    "    \"min_child_weight\": 1,\n",
    "    \"colsample_bytree\": 1,\n",
    "    \"colsample_bylevel\": 1\n",
    "}\n",
    "n_boost = 10\n",
    "# Fit the model to each dataset\n",
    "mlist = []\n",
    "for i in range(num_datasets):\n",
    "    bst = xgb.cv(dval, dmat_list[i], num_boost_round=n_boost, nfold=5, metrics='rmse', seed=123, shuffle=True)\n",
    "    mlist.append(bst.iloc[len(bst.index)-1, 2])\n",
    "def_result[\"xgb_rmse\"] = mlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of rmse: \n",
      " rf_rmse     7.188438\n",
      "xgb_rmse    6.960066\n",
      "dtype: float64 \n",
      "\n",
      "Standard deviation of rmse: \n",
      " rf_rmse     2.823852\n",
      "xgb_rmse    3.653274\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean of rmse: \\n\",def_result.mean(axis=0),\"\\n\")\n",
    "print(\"Standard deviation of rmse: \\n\",def_result.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of default values: 7.188437947238364 \n",
      "\n",
      "RMSE of default values: 6.960065807407409\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE of default values: {}\".format(def_result.mean(axis=0)[0]), \"\\n\")\n",
    "print(\"RMSE of default values: {}\".format(def_result.mean(axis=0)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f82f2301410>"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dv_plot = def_result.boxplot()\n",
    "dv_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tunability of single hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# n_estimators\n",
    "target = rf_n_estimators\n",
    "# Best hyperparameter\n",
    "best = target[\"value\"].idxmin()\n",
    "param = {'n_estimators': target.iloc[best,:][5],\n",
    "         'criterion': 'mse',\n",
    "         'bootstrap': True,\n",
    "         'max_samples': None,\n",
    "         'max_features': 'sqrt',\n",
    "         'min_samples_leaf': 1}\n",
    "# Fit the model to each datsets\n",
    "rf_tune_one = pd.DataFrame(columns=['n_estimators'])\n",
    "mlist = []\n",
    "mod = RandomForestRegressor(**param)\n",
    "for i in range(num_datasets):\n",
    "    num_col = df_list[i].shape[1]\n",
    "    yvec = df_list[i].iloc[:,0:1].values.ravel()\n",
    "    rmse = cross_val_score(mod, df_list[i].iloc[:,1:num_col], yvec, scoring=\"neg_root_mean_squared_error\", cv=5)\n",
    "    mlist.append(abs(rmse).mean())\n",
    "rf_tune_one['n_estimators'] = mlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrap\n",
    "target = rf_bootstrap\n",
    "# Best hyperparameter\n",
    "best = target[\"value\"].idxmin()\n",
    "val = target.iloc[best,:][5]\n",
    "param = {'n_estimators': 100,\n",
    "         'criterion': 'mse',\n",
    "         'bootstrap': val,\n",
    "         'max_samples': None,\n",
    "         'max_features': 'sqrt',\n",
    "         'min_samples_leaf': 1}\n",
    "# Fit the model to each datsets\n",
    "mlist = []\n",
    "mod = RandomForestRegressor(**param)\n",
    "for i in range(num_datasets):\n",
    "    num_col = df_list[i].shape[1]\n",
    "    yvec = df_list[i].iloc[:,0:1].values.ravel()\n",
    "    rmse = cross_val_score(mod, df_list[i].iloc[:,1:num_col], yvec, scoring=\"neg_root_mean_squared_error\", cv=5)\n",
    "    mlist.append(abs(rmse).mean())\n",
    "rf_tune_one['bootstrap'] = mlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_samples\n",
    "target = rf_max_samples\n",
    "# Best hyperparameter\n",
    "best = target[\"value\"].idxmin()\n",
    "val = target.iloc[best,:][5]\n",
    "param = {'n_estimators': 100,\n",
    "         'criterion': 'mse',\n",
    "         'bootstrap': True,\n",
    "         'max_samples': val,\n",
    "         'max_features': 'sqrt',\n",
    "         'min_samples_leaf': 1}\n",
    "# Fit the model to each datsets\n",
    "mlist = []\n",
    "mod = RandomForestRegressor(**param)\n",
    "for i in range(num_datasets):\n",
    "    num_col = df_list[i].shape[1]\n",
    "    yvec = df_list[i].iloc[:,0:1].values.ravel()\n",
    "    rmse = cross_val_score(mod, df_list[i].iloc[:,1:num_col], yvec, scoring=\"neg_root_mean_squared_error\", cv=5)\n",
    "    mlist.append(abs(rmse).mean())\n",
    "rf_tune_one['max_samples'] = mlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_features\n",
    "target = rf_max_features\n",
    "# Best hyperparameter\n",
    "best = target[\"value\"].idxmin()\n",
    "val = target.iloc[best,:][5]\n",
    "param = {'n_estimators': 100,\n",
    "         'criterion': 'mse',\n",
    "         'bootstrap': True,\n",
    "         'max_samples': None,\n",
    "         'max_features': val,\n",
    "         'min_samples_leaf': 1}\n",
    "# Fit the model to each datsets\n",
    "mlist = []\n",
    "mod = RandomForestRegressor(**param)\n",
    "for i in range(num_datasets):\n",
    "    num_col = df_list[i].shape[1]\n",
    "    yvec = df_list[i].iloc[:,0:1].values.ravel()\n",
    "    rmse = cross_val_score(mod, df_list[i].iloc[:,1:num_col], yvec, scoring=\"neg_root_mean_squared_error\", cv=5)\n",
    "    mlist.append(abs(rmse).mean())\n",
    "rf_tune_one['max_features'] = mlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_samples_leaf\n",
    "target = rf_min_samples_leaf\n",
    "# Best hyperparameter\n",
    "best = target[\"value\"].idxmin()\n",
    "val = target.iloc[best,:][5]\n",
    "param = {'n_estimators': 100,\n",
    "         'criterion': 'mse',\n",
    "         'bootstrap': True,\n",
    "         'max_samples': None,\n",
    "         'max_features': 'sqrt',\n",
    "         'min_samples_leaf': int(val)}\n",
    "# Fit the model to each datsets\n",
    "mlist = []\n",
    "mod = RandomForestRegressor(**param)\n",
    "for i in range(num_datasets):\n",
    "    num_col = df_list[i].shape[1]\n",
    "    yvec = df_list[i].iloc[:,0:1].values.ravel()\n",
    "    rmse = cross_val_score(mod, df_list[i].iloc[:,1:num_col], yvec, scoring=\"neg_root_mean_squared_error\", cv=5)\n",
    "    mlist.append(abs(rmse).mean())\n",
    "rf_tune_one['min_samples_leaf'] = mlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>bootstrap</th>\n",
       "      <th>max_samples</th>\n",
       "      <th>max_features</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.164081</td>\n",
       "      <td>4.163676</td>\n",
       "      <td>4.193671</td>\n",
       "      <td>4.182185</td>\n",
       "      <td>3.774048</td>\n",
       "      <td>4.148849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.486712</td>\n",
       "      <td>6.406425</td>\n",
       "      <td>6.651739</td>\n",
       "      <td>6.532629</td>\n",
       "      <td>6.524477</td>\n",
       "      <td>6.435129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.068842</td>\n",
       "      <td>11.079978</td>\n",
       "      <td>11.096068</td>\n",
       "      <td>11.096376</td>\n",
       "      <td>10.901847</td>\n",
       "      <td>10.894888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.779594</td>\n",
       "      <td>4.723038</td>\n",
       "      <td>4.704867</td>\n",
       "      <td>4.764991</td>\n",
       "      <td>4.468592</td>\n",
       "      <td>4.728093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.264643</td>\n",
       "      <td>7.150328</td>\n",
       "      <td>7.198562</td>\n",
       "      <td>7.099187</td>\n",
       "      <td>7.172794</td>\n",
       "      <td>7.035567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11.419200</td>\n",
       "      <td>11.548048</td>\n",
       "      <td>11.736723</td>\n",
       "      <td>11.656355</td>\n",
       "      <td>11.733810</td>\n",
       "      <td>11.392738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.745277</td>\n",
       "      <td>4.710559</td>\n",
       "      <td>4.697017</td>\n",
       "      <td>4.694210</td>\n",
       "      <td>3.735217</td>\n",
       "      <td>4.654887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6.116832</td>\n",
       "      <td>6.114048</td>\n",
       "      <td>6.058755</td>\n",
       "      <td>6.104159</td>\n",
       "      <td>5.983225</td>\n",
       "      <td>6.030611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.634861</td>\n",
       "      <td>9.595652</td>\n",
       "      <td>9.762372</td>\n",
       "      <td>9.787630</td>\n",
       "      <td>9.426359</td>\n",
       "      <td>9.716546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.502886</td>\n",
       "      <td>3.537832</td>\n",
       "      <td>3.346443</td>\n",
       "      <td>3.534064</td>\n",
       "      <td>2.525076</td>\n",
       "      <td>3.526538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6.027999</td>\n",
       "      <td>5.955533</td>\n",
       "      <td>5.863668</td>\n",
       "      <td>5.943890</td>\n",
       "      <td>5.506695</td>\n",
       "      <td>5.953833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10.258315</td>\n",
       "      <td>10.200577</td>\n",
       "      <td>10.126520</td>\n",
       "      <td>10.225814</td>\n",
       "      <td>10.056509</td>\n",
       "      <td>10.206290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.384851</td>\n",
       "      <td>4.343692</td>\n",
       "      <td>4.224980</td>\n",
       "      <td>4.369348</td>\n",
       "      <td>2.858566</td>\n",
       "      <td>4.392792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6.667586</td>\n",
       "      <td>6.661060</td>\n",
       "      <td>6.587519</td>\n",
       "      <td>6.667215</td>\n",
       "      <td>6.244128</td>\n",
       "      <td>6.639744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>11.311921</td>\n",
       "      <td>11.321257</td>\n",
       "      <td>11.485490</td>\n",
       "      <td>11.231332</td>\n",
       "      <td>11.335533</td>\n",
       "      <td>11.400954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4.624345</td>\n",
       "      <td>4.590248</td>\n",
       "      <td>4.543847</td>\n",
       "      <td>4.590932</td>\n",
       "      <td>2.827803</td>\n",
       "      <td>4.634525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6.809632</td>\n",
       "      <td>6.810241</td>\n",
       "      <td>6.696040</td>\n",
       "      <td>6.805608</td>\n",
       "      <td>5.975824</td>\n",
       "      <td>6.862131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>11.101216</td>\n",
       "      <td>11.079072</td>\n",
       "      <td>11.033072</td>\n",
       "      <td>11.085944</td>\n",
       "      <td>10.889546</td>\n",
       "      <td>11.186525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3.326082</td>\n",
       "      <td>3.314442</td>\n",
       "      <td>3.182095</td>\n",
       "      <td>3.317271</td>\n",
       "      <td>2.304925</td>\n",
       "      <td>3.334296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6.034202</td>\n",
       "      <td>5.959943</td>\n",
       "      <td>5.840049</td>\n",
       "      <td>6.029467</td>\n",
       "      <td>5.488845</td>\n",
       "      <td>5.989612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10.618502</td>\n",
       "      <td>10.542963</td>\n",
       "      <td>10.438994</td>\n",
       "      <td>10.682251</td>\n",
       "      <td>10.365778</td>\n",
       "      <td>10.673531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4.184435</td>\n",
       "      <td>4.122852</td>\n",
       "      <td>3.998499</td>\n",
       "      <td>4.172933</td>\n",
       "      <td>2.587184</td>\n",
       "      <td>4.134873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6.408757</td>\n",
       "      <td>6.334698</td>\n",
       "      <td>6.284485</td>\n",
       "      <td>6.412189</td>\n",
       "      <td>5.600712</td>\n",
       "      <td>6.380709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10.734789</td>\n",
       "      <td>10.678165</td>\n",
       "      <td>10.636306</td>\n",
       "      <td>10.642028</td>\n",
       "      <td>10.386308</td>\n",
       "      <td>10.684767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.420722</td>\n",
       "      <td>4.402335</td>\n",
       "      <td>4.360067</td>\n",
       "      <td>4.401397</td>\n",
       "      <td>2.721302</td>\n",
       "      <td>4.408734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6.772333</td>\n",
       "      <td>6.747460</td>\n",
       "      <td>6.696268</td>\n",
       "      <td>6.803261</td>\n",
       "      <td>5.898493</td>\n",
       "      <td>6.757733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>11.219207</td>\n",
       "      <td>11.174775</td>\n",
       "      <td>11.159015</td>\n",
       "      <td>11.201819</td>\n",
       "      <td>10.830799</td>\n",
       "      <td>11.168661</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      default  n_estimators  bootstrap  max_samples  max_features  \\\n",
       "0    4.164081      4.163676   4.193671     4.182185      3.774048   \n",
       "1    6.486712      6.406425   6.651739     6.532629      6.524477   \n",
       "2   11.068842     11.079978  11.096068    11.096376     10.901847   \n",
       "3    4.779594      4.723038   4.704867     4.764991      4.468592   \n",
       "4    7.264643      7.150328   7.198562     7.099187      7.172794   \n",
       "5   11.419200     11.548048  11.736723    11.656355     11.733810   \n",
       "6    4.745277      4.710559   4.697017     4.694210      3.735217   \n",
       "7    6.116832      6.114048   6.058755     6.104159      5.983225   \n",
       "8    9.634861      9.595652   9.762372     9.787630      9.426359   \n",
       "9    3.502886      3.537832   3.346443     3.534064      2.525076   \n",
       "10   6.027999      5.955533   5.863668     5.943890      5.506695   \n",
       "11  10.258315     10.200577  10.126520    10.225814     10.056509   \n",
       "12   4.384851      4.343692   4.224980     4.369348      2.858566   \n",
       "13   6.667586      6.661060   6.587519     6.667215      6.244128   \n",
       "14  11.311921     11.321257  11.485490    11.231332     11.335533   \n",
       "15   4.624345      4.590248   4.543847     4.590932      2.827803   \n",
       "16   6.809632      6.810241   6.696040     6.805608      5.975824   \n",
       "17  11.101216     11.079072  11.033072    11.085944     10.889546   \n",
       "18   3.326082      3.314442   3.182095     3.317271      2.304925   \n",
       "19   6.034202      5.959943   5.840049     6.029467      5.488845   \n",
       "20  10.618502     10.542963  10.438994    10.682251     10.365778   \n",
       "21   4.184435      4.122852   3.998499     4.172933      2.587184   \n",
       "22   6.408757      6.334698   6.284485     6.412189      5.600712   \n",
       "23  10.734789     10.678165  10.636306    10.642028     10.386308   \n",
       "24   4.420722      4.402335   4.360067     4.401397      2.721302   \n",
       "25   6.772333      6.747460   6.696268     6.803261      5.898493   \n",
       "26  11.219207     11.174775  11.159015    11.201819     10.830799   \n",
       "\n",
       "    min_samples_leaf  \n",
       "0           4.148849  \n",
       "1           6.435129  \n",
       "2          10.894888  \n",
       "3           4.728093  \n",
       "4           7.035567  \n",
       "5          11.392738  \n",
       "6           4.654887  \n",
       "7           6.030611  \n",
       "8           9.716546  \n",
       "9           3.526538  \n",
       "10          5.953833  \n",
       "11         10.206290  \n",
       "12          4.392792  \n",
       "13          6.639744  \n",
       "14         11.400954  \n",
       "15          4.634525  \n",
       "16          6.862131  \n",
       "17         11.186525  \n",
       "18          3.334296  \n",
       "19          5.989612  \n",
       "20         10.673531  \n",
       "21          4.134873  \n",
       "22          6.380709  \n",
       "23         10.684767  \n",
       "24          4.408734  \n",
       "25          6.757733  \n",
       "26         11.168661  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_tune_one.insert(0, column='default', value=def_result.iloc[:,0])\n",
    "rf_tune_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of rmse:\n",
      " default             7.188438\n",
      "n_estimators        7.158107\n",
      "bootstrap           7.133449\n",
      "max_samples         7.186462\n",
      "max_features        6.597200\n",
      "min_samples_leaf    7.161984\n",
      "dtype: float64 \n",
      "\n",
      "Std of rmse: \n",
      " default             2.823852\n",
      "n_estimators        2.831001\n",
      "bootstrap           2.879002\n",
      "max_samples         2.839927\n",
      "max_features        3.236536\n",
      "min_samples_leaf    2.830730\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean of rmse:\\n\", rf_tune_one.mean(), \"\\n\")\n",
    "print(\"Std of rmse: \\n\", rf_tune_one.std())\n",
    "rf_tune_one.to_csv(\"rf_tune_one.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f82ecd3fb90>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rf_tune_one.boxplot(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_tune_one = pd.DataFrame(data=def_result.iloc[:,1].values.ravel(), columns=['default'])\n",
    "# lambda\n",
    "target = xgb_lambda\n",
    "# Best hyperparameter\n",
    "best = target[\"value\"].idxmin()\n",
    "val = target.iloc[best,:][5]\n",
    "param = {\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"lambda\": val,\n",
    "    \"alpha\": 0,\n",
    "    \"eta\": 0.3,\n",
    "    \"subsample\": 1,\n",
    "    \"max_depth\": 6,\n",
    "    \"min_child_weight\": 1,\n",
    "    \"colsample_bytree\": 1,\n",
    "    \"colsample_bylevel\": 1\n",
    "}\n",
    "n_boost = 10\n",
    "# Fit the model to each dataset\n",
    "mlist = []\n",
    "for i in range(num_datasets):\n",
    "    bst = xgb.cv(param, dmat_list[i], num_boost_round=n_boost, nfold=5, metrics='rmse', seed=123, shuffle=True)\n",
    "    mlist.append(bst.iloc[len(bst.index)-1, 2])\n",
    "xgb_tune_one[\"lambda\"] = mlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha\n",
    "target = xgb_alpha\n",
    "# Best hyperparameter\n",
    "best = target[\"value\"].idxmin()\n",
    "val = target.iloc[best,:][5]\n",
    "param = {\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"lambda\": 1,\n",
    "    \"alpha\": val,\n",
    "    \"eta\": 0.3,\n",
    "    \"subsample\": 1,\n",
    "    \"max_depth\": 6,\n",
    "    \"min_child_weight\": 1,\n",
    "    \"colsample_bytree\": 1,\n",
    "    \"colsample_bylevel\": 1\n",
    "}\n",
    "n_boost = 10\n",
    "# Fit the model to each dataset\n",
    "mlist = []\n",
    "for i in range(num_datasets):\n",
    "    bst = xgb.cv(param, dmat_list[i], num_boost_round=n_boost, nfold=5, metrics='rmse', seed=123, shuffle=True)\n",
    "    mlist.append(bst.iloc[len(bst.index)-1, 2])\n",
    "xgb_tune_one[\"alpha\"] = mlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eta\n",
    "target = xgb_eta\n",
    "# Best hyperparameter\n",
    "best = target[\"value\"].idxmin()\n",
    "val = target.iloc[best,:][5]\n",
    "param = {\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"lambda\": 1,\n",
    "    \"alpha\": 0,\n",
    "    \"eta\": val,\n",
    "    \"subsample\": 1,\n",
    "    \"max_depth\": 6,\n",
    "    \"min_child_weight\": 1,\n",
    "    \"colsample_bytree\": 1,\n",
    "    \"colsample_bylevel\": 1\n",
    "}\n",
    "n_boost = 10\n",
    "# Fit the model to each dataset\n",
    "mlist = []\n",
    "for i in range(num_datasets):\n",
    "    bst = xgb.cv(param, dmat_list[i], num_boost_round=n_boost, nfold=5, metrics='rmse', seed=123, shuffle=True)\n",
    "    mlist.append(bst.iloc[len(bst.index)-1, 2])\n",
    "xgb_tune_one[\"eta\"] = mlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsample\n",
    "target = xgb_subsample\n",
    "# Best hyperparameter\n",
    "best = target[\"value\"].idxmin()\n",
    "val = target.iloc[best,:][5]\n",
    "param = {\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"lambda\": 1,\n",
    "    \"alpha\": 0,\n",
    "    \"eta\": 0.3,\n",
    "    \"subsample\": val,\n",
    "    \"max_depth\": 6,\n",
    "    \"min_child_weight\": 1,\n",
    "    \"colsample_bytree\": 1,\n",
    "    \"colsample_bylevel\": 1\n",
    "}\n",
    "n_boost = 10\n",
    "# Fit the model to each dataset\n",
    "mlist = []\n",
    "for i in range(num_datasets):\n",
    "    bst = xgb.cv(param, dmat_list[i], num_boost_round=n_boost, nfold=5, metrics='rmse', seed=123, shuffle=True)\n",
    "    mlist.append(bst.iloc[len(bst.index)-1, 2])\n",
    "xgb_tune_one[\"subsample\"] = mlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_depth\n",
    "target = xgb_max_depth\n",
    "# Best hyperparameter\n",
    "best = target[\"value\"].idxmin()\n",
    "val = target.iloc[best,:][5]\n",
    "param = {\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"lambda\": 1,\n",
    "    \"alpha\": 0,\n",
    "    \"eta\": 0.3,\n",
    "    \"subsample\": 1,\n",
    "    \"max_depth\": int(val),\n",
    "    \"min_child_weight\": 1,\n",
    "    \"colsample_bytree\": 1,\n",
    "    \"colsample_bylevel\": 1\n",
    "}\n",
    "n_boost = 10\n",
    "# Fit the model to each dataset\n",
    "mlist = []\n",
    "for i in range(num_datasets):\n",
    "    bst = xgb.cv(param, dmat_list[i], num_boost_round=n_boost, nfold=5, metrics='rmse', seed=123, shuffle=True)\n",
    "    mlist.append(bst.iloc[len(bst.index)-1, 2])\n",
    "xgb_tune_one[\"max_depth\"] = mlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_child_weight\n",
    "target = xgb_min_child_weight\n",
    "# Best hyperparameter\n",
    "best = target[\"value\"].idxmin()\n",
    "val = target.iloc[best,:][5]\n",
    "param = {\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"lambda\": 1,\n",
    "    \"alpha\": 0,\n",
    "    \"eta\": 0.3,\n",
    "    \"subsample\": 1,\n",
    "    \"max_depth\": 6,\n",
    "    \"min_child_weight\": int(val),\n",
    "    \"colsample_bytree\": 1,\n",
    "    \"colsample_bylevel\": 1\n",
    "}\n",
    "n_boost = 10\n",
    "# Fit the model to each dataset\n",
    "mlist = []\n",
    "for i in range(num_datasets):\n",
    "    bst = xgb.cv(param, dmat_list[i], num_boost_round=n_boost, nfold=5, metrics='rmse', seed=123, shuffle=True)\n",
    "    mlist.append(bst.iloc[len(bst.index)-1, 2])\n",
    "xgb_tune_one[\"min_child_weight\"] = mlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colsample_bytree\n",
    "target = xgb_colsample_bytree\n",
    "# Best hyperparameter\n",
    "best = target[\"value\"].idxmin()\n",
    "val = target.iloc[best,:][5]\n",
    "param = {\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"lambda\": 1,\n",
    "    \"alpha\": 0,\n",
    "    \"eta\": 0.3,\n",
    "    \"subsample\": 1,\n",
    "    \"max_depth\": 6,\n",
    "    \"min_child_weight\": 1,\n",
    "    \"colsample_bytree\": val,\n",
    "    \"colsample_bylevel\": 1\n",
    "}\n",
    "n_boost = 10\n",
    "# Fit the model to each dataset\n",
    "mlist = []\n",
    "for i in range(num_datasets):\n",
    "    bst = xgb.cv(param, dmat_list[i], num_boost_round=n_boost, nfold=5, metrics='rmse', seed=123, shuffle=True)\n",
    "    mlist.append(bst.iloc[len(bst.index)-1, 2])\n",
    "xgb_tune_one[\"colsample_bytree\"] = mlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colsample_bylevel\n",
    "target = xgb_colsample_bylevel\n",
    "# Best hyperparameter\n",
    "best = target[\"value\"].idxmin()\n",
    "val = target.iloc[best,:][5]\n",
    "param = {\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"lambda\": 1,\n",
    "    \"alpha\": 0,\n",
    "    \"eta\": 0.3,\n",
    "    \"subsample\": 1,\n",
    "    \"max_depth\": 6,\n",
    "    \"min_child_weight\": 1,\n",
    "    \"colsample_bytree\": 1,\n",
    "    \"colsample_bylevel\": val\n",
    "}\n",
    "n_boost = 10\n",
    "# Fit the model to each dataset\n",
    "mlist = []\n",
    "for i in range(num_datasets):\n",
    "    bst = xgb.cv(param, dmat_list[i], num_boost_round=n_boost, nfold=5, metrics='rmse', seed=123, shuffle=True)\n",
    "    mlist.append(bst.iloc[len(bst.index)-1, 2])\n",
    "xgb_tune_one[\"colsample_bylevel\"] = mlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_boost_round\n",
    "target = xgb_num_boost_round\n",
    "# Best hyperparameter\n",
    "best = target[\"value\"].idxmin()\n",
    "val = target.iloc[best,:][5]\n",
    "param = {\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"lambda\": 1,\n",
    "    \"alpha\": 0,\n",
    "    \"eta\": 0.3,\n",
    "    \"subsample\": 1,\n",
    "    \"max_depth\": 6,\n",
    "    \"min_child_weight\": 1,\n",
    "    \"colsample_bytree\": 1,\n",
    "    \"colsample_bylevel\": 1\n",
    "}\n",
    "n_boost = val\n",
    "# Fit the model to each dataset\n",
    "mlist = []\n",
    "for i in range(num_datasets):\n",
    "    bst = xgb.cv(param, dmat_list[i], num_boost_round=n_boost, nfold=5, metrics='rmse', seed=123, shuffle=True)\n",
    "    mlist.append(bst.iloc[len(bst.index)-1, 2])\n",
    "xgb_tune_one.insert(1, column=\"num_boost_round\",value=mlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default</th>\n",
       "      <th>num_boost_round</th>\n",
       "      <th>lambda</th>\n",
       "      <th>alpha</th>\n",
       "      <th>eta</th>\n",
       "      <th>subsample</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>colsample_bylevel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.485980</td>\n",
       "      <td>3.442551</td>\n",
       "      <td>3.673997</td>\n",
       "      <td>3.415693</td>\n",
       "      <td>3.485980</td>\n",
       "      <td>3.485980</td>\n",
       "      <td>3.199025</td>\n",
       "      <td>4.757930</td>\n",
       "      <td>3.485980</td>\n",
       "      <td>3.705964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.950154</td>\n",
       "      <td>6.907205</td>\n",
       "      <td>7.109430</td>\n",
       "      <td>6.903711</td>\n",
       "      <td>6.950154</td>\n",
       "      <td>6.950154</td>\n",
       "      <td>6.681826</td>\n",
       "      <td>6.424638</td>\n",
       "      <td>6.950154</td>\n",
       "      <td>6.826435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.921989</td>\n",
       "      <td>11.954127</td>\n",
       "      <td>12.064674</td>\n",
       "      <td>11.709138</td>\n",
       "      <td>11.921989</td>\n",
       "      <td>11.921989</td>\n",
       "      <td>11.811400</td>\n",
       "      <td>10.667291</td>\n",
       "      <td>11.921989</td>\n",
       "      <td>11.566980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.610960</td>\n",
       "      <td>4.577426</td>\n",
       "      <td>4.690707</td>\n",
       "      <td>4.639455</td>\n",
       "      <td>4.610960</td>\n",
       "      <td>4.610960</td>\n",
       "      <td>3.965434</td>\n",
       "      <td>4.912964</td>\n",
       "      <td>4.610960</td>\n",
       "      <td>4.467350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.602259</td>\n",
       "      <td>7.511600</td>\n",
       "      <td>7.279410</td>\n",
       "      <td>7.607000</td>\n",
       "      <td>7.602259</td>\n",
       "      <td>7.602259</td>\n",
       "      <td>8.025462</td>\n",
       "      <td>7.297839</td>\n",
       "      <td>7.602259</td>\n",
       "      <td>7.411149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13.823878</td>\n",
       "      <td>14.021954</td>\n",
       "      <td>12.762547</td>\n",
       "      <td>13.683796</td>\n",
       "      <td>13.823878</td>\n",
       "      <td>13.823878</td>\n",
       "      <td>12.907924</td>\n",
       "      <td>11.603148</td>\n",
       "      <td>13.823878</td>\n",
       "      <td>12.463672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.441053</td>\n",
       "      <td>3.431942</td>\n",
       "      <td>3.739486</td>\n",
       "      <td>3.454440</td>\n",
       "      <td>3.441053</td>\n",
       "      <td>3.441053</td>\n",
       "      <td>3.225302</td>\n",
       "      <td>4.863938</td>\n",
       "      <td>3.441053</td>\n",
       "      <td>3.675041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.706423</td>\n",
       "      <td>5.701820</td>\n",
       "      <td>6.045979</td>\n",
       "      <td>5.632324</td>\n",
       "      <td>5.706423</td>\n",
       "      <td>5.706423</td>\n",
       "      <td>5.730378</td>\n",
       "      <td>6.241171</td>\n",
       "      <td>5.706423</td>\n",
       "      <td>5.897558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11.041093</td>\n",
       "      <td>11.125039</td>\n",
       "      <td>10.180957</td>\n",
       "      <td>10.750389</td>\n",
       "      <td>11.041093</td>\n",
       "      <td>11.041093</td>\n",
       "      <td>10.478511</td>\n",
       "      <td>9.779190</td>\n",
       "      <td>11.041093</td>\n",
       "      <td>9.981541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.613650</td>\n",
       "      <td>2.447711</td>\n",
       "      <td>2.623967</td>\n",
       "      <td>2.633333</td>\n",
       "      <td>2.613650</td>\n",
       "      <td>2.613650</td>\n",
       "      <td>2.582229</td>\n",
       "      <td>2.379361</td>\n",
       "      <td>2.613650</td>\n",
       "      <td>2.865350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.764146</td>\n",
       "      <td>5.715799</td>\n",
       "      <td>5.617598</td>\n",
       "      <td>5.663887</td>\n",
       "      <td>5.764146</td>\n",
       "      <td>5.764146</td>\n",
       "      <td>5.315068</td>\n",
       "      <td>5.289505</td>\n",
       "      <td>5.764146</td>\n",
       "      <td>5.927282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10.547343</td>\n",
       "      <td>10.583226</td>\n",
       "      <td>10.355571</td>\n",
       "      <td>10.571016</td>\n",
       "      <td>10.547343</td>\n",
       "      <td>10.547343</td>\n",
       "      <td>9.824290</td>\n",
       "      <td>9.868765</td>\n",
       "      <td>10.547343</td>\n",
       "      <td>10.529860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.956695</td>\n",
       "      <td>2.809000</td>\n",
       "      <td>3.118362</td>\n",
       "      <td>2.869494</td>\n",
       "      <td>2.956695</td>\n",
       "      <td>2.956695</td>\n",
       "      <td>2.615771</td>\n",
       "      <td>2.456955</td>\n",
       "      <td>2.956695</td>\n",
       "      <td>3.056532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6.710687</td>\n",
       "      <td>6.682097</td>\n",
       "      <td>6.573259</td>\n",
       "      <td>6.755268</td>\n",
       "      <td>6.710687</td>\n",
       "      <td>6.710687</td>\n",
       "      <td>6.136140</td>\n",
       "      <td>6.167112</td>\n",
       "      <td>6.710687</td>\n",
       "      <td>6.581566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>11.786248</td>\n",
       "      <td>11.831233</td>\n",
       "      <td>11.850024</td>\n",
       "      <td>11.692454</td>\n",
       "      <td>11.786248</td>\n",
       "      <td>11.786248</td>\n",
       "      <td>11.396620</td>\n",
       "      <td>11.511399</td>\n",
       "      <td>11.786248</td>\n",
       "      <td>11.637370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.922296</td>\n",
       "      <td>2.777937</td>\n",
       "      <td>3.093861</td>\n",
       "      <td>2.909988</td>\n",
       "      <td>2.922296</td>\n",
       "      <td>2.922296</td>\n",
       "      <td>2.632151</td>\n",
       "      <td>2.375401</td>\n",
       "      <td>2.922296</td>\n",
       "      <td>3.262853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6.353111</td>\n",
       "      <td>6.316472</td>\n",
       "      <td>6.455862</td>\n",
       "      <td>6.335137</td>\n",
       "      <td>6.353111</td>\n",
       "      <td>6.353111</td>\n",
       "      <td>5.923493</td>\n",
       "      <td>5.864748</td>\n",
       "      <td>6.353111</td>\n",
       "      <td>6.204867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>11.581193</td>\n",
       "      <td>11.568513</td>\n",
       "      <td>11.686197</td>\n",
       "      <td>11.613925</td>\n",
       "      <td>11.581193</td>\n",
       "      <td>11.581193</td>\n",
       "      <td>10.885395</td>\n",
       "      <td>10.940802</td>\n",
       "      <td>11.581193</td>\n",
       "      <td>11.552360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.221516</td>\n",
       "      <td>2.061082</td>\n",
       "      <td>2.345231</td>\n",
       "      <td>2.259582</td>\n",
       "      <td>2.221516</td>\n",
       "      <td>2.221516</td>\n",
       "      <td>2.519764</td>\n",
       "      <td>2.031744</td>\n",
       "      <td>2.221516</td>\n",
       "      <td>2.477166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.719310</td>\n",
       "      <td>5.697141</td>\n",
       "      <td>5.775371</td>\n",
       "      <td>5.774303</td>\n",
       "      <td>5.719310</td>\n",
       "      <td>5.719310</td>\n",
       "      <td>5.527250</td>\n",
       "      <td>5.520651</td>\n",
       "      <td>5.719310</td>\n",
       "      <td>5.832009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10.830093</td>\n",
       "      <td>10.962470</td>\n",
       "      <td>10.719173</td>\n",
       "      <td>10.812960</td>\n",
       "      <td>10.830093</td>\n",
       "      <td>10.830093</td>\n",
       "      <td>10.315308</td>\n",
       "      <td>10.551416</td>\n",
       "      <td>10.830093</td>\n",
       "      <td>10.856750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.432251</td>\n",
       "      <td>2.293813</td>\n",
       "      <td>2.532328</td>\n",
       "      <td>2.414390</td>\n",
       "      <td>2.432251</td>\n",
       "      <td>2.432251</td>\n",
       "      <td>2.494361</td>\n",
       "      <td>2.184076</td>\n",
       "      <td>2.432251</td>\n",
       "      <td>2.620945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5.969285</td>\n",
       "      <td>5.962942</td>\n",
       "      <td>5.878899</td>\n",
       "      <td>5.955066</td>\n",
       "      <td>5.969285</td>\n",
       "      <td>5.969285</td>\n",
       "      <td>5.532605</td>\n",
       "      <td>5.512678</td>\n",
       "      <td>5.969285</td>\n",
       "      <td>5.918731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10.887201</td>\n",
       "      <td>10.969157</td>\n",
       "      <td>11.063458</td>\n",
       "      <td>10.837431</td>\n",
       "      <td>10.887201</td>\n",
       "      <td>10.887201</td>\n",
       "      <td>10.468751</td>\n",
       "      <td>10.683931</td>\n",
       "      <td>10.887201</td>\n",
       "      <td>11.002824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2.501507</td>\n",
       "      <td>2.358697</td>\n",
       "      <td>2.702608</td>\n",
       "      <td>2.515150</td>\n",
       "      <td>2.501507</td>\n",
       "      <td>2.501507</td>\n",
       "      <td>2.617337</td>\n",
       "      <td>2.266223</td>\n",
       "      <td>2.501507</td>\n",
       "      <td>2.955923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6.148260</td>\n",
       "      <td>6.121690</td>\n",
       "      <td>6.205812</td>\n",
       "      <td>6.178036</td>\n",
       "      <td>6.148260</td>\n",
       "      <td>6.148260</td>\n",
       "      <td>5.831188</td>\n",
       "      <td>5.814391</td>\n",
       "      <td>6.148260</td>\n",
       "      <td>6.222434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>11.393198</td>\n",
       "      <td>11.433957</td>\n",
       "      <td>11.423315</td>\n",
       "      <td>11.554053</td>\n",
       "      <td>11.393198</td>\n",
       "      <td>11.393198</td>\n",
       "      <td>10.881552</td>\n",
       "      <td>11.049647</td>\n",
       "      <td>11.393198</td>\n",
       "      <td>11.644470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      default  num_boost_round     lambda      alpha        eta  subsample  \\\n",
       "0    3.485980         3.442551   3.673997   3.415693   3.485980   3.485980   \n",
       "1    6.950154         6.907205   7.109430   6.903711   6.950154   6.950154   \n",
       "2   11.921989        11.954127  12.064674  11.709138  11.921989  11.921989   \n",
       "3    4.610960         4.577426   4.690707   4.639455   4.610960   4.610960   \n",
       "4    7.602259         7.511600   7.279410   7.607000   7.602259   7.602259   \n",
       "5   13.823878        14.021954  12.762547  13.683796  13.823878  13.823878   \n",
       "6    3.441053         3.431942   3.739486   3.454440   3.441053   3.441053   \n",
       "7    5.706423         5.701820   6.045979   5.632324   5.706423   5.706423   \n",
       "8   11.041093        11.125039  10.180957  10.750389  11.041093  11.041093   \n",
       "9    2.613650         2.447711   2.623967   2.633333   2.613650   2.613650   \n",
       "10   5.764146         5.715799   5.617598   5.663887   5.764146   5.764146   \n",
       "11  10.547343        10.583226  10.355571  10.571016  10.547343  10.547343   \n",
       "12   2.956695         2.809000   3.118362   2.869494   2.956695   2.956695   \n",
       "13   6.710687         6.682097   6.573259   6.755268   6.710687   6.710687   \n",
       "14  11.786248        11.831233  11.850024  11.692454  11.786248  11.786248   \n",
       "15   2.922296         2.777937   3.093861   2.909988   2.922296   2.922296   \n",
       "16   6.353111         6.316472   6.455862   6.335137   6.353111   6.353111   \n",
       "17  11.581193        11.568513  11.686197  11.613925  11.581193  11.581193   \n",
       "18   2.221516         2.061082   2.345231   2.259582   2.221516   2.221516   \n",
       "19   5.719310         5.697141   5.775371   5.774303   5.719310   5.719310   \n",
       "20  10.830093        10.962470  10.719173  10.812960  10.830093  10.830093   \n",
       "21   2.432251         2.293813   2.532328   2.414390   2.432251   2.432251   \n",
       "22   5.969285         5.962942   5.878899   5.955066   5.969285   5.969285   \n",
       "23  10.887201        10.969157  11.063458  10.837431  10.887201  10.887201   \n",
       "24   2.501507         2.358697   2.702608   2.515150   2.501507   2.501507   \n",
       "25   6.148260         6.121690   6.205812   6.178036   6.148260   6.148260   \n",
       "26  11.393198        11.433957  11.423315  11.554053  11.393198  11.393198   \n",
       "\n",
       "    max_depth  min_child_weight  colsample_bytree  colsample_bylevel  \n",
       "0    3.199025          4.757930          3.485980           3.705964  \n",
       "1    6.681826          6.424638          6.950154           6.826435  \n",
       "2   11.811400         10.667291         11.921989          11.566980  \n",
       "3    3.965434          4.912964          4.610960           4.467350  \n",
       "4    8.025462          7.297839          7.602259           7.411149  \n",
       "5   12.907924         11.603148         13.823878          12.463672  \n",
       "6    3.225302          4.863938          3.441053           3.675041  \n",
       "7    5.730378          6.241171          5.706423           5.897558  \n",
       "8   10.478511          9.779190         11.041093           9.981541  \n",
       "9    2.582229          2.379361          2.613650           2.865350  \n",
       "10   5.315068          5.289505          5.764146           5.927282  \n",
       "11   9.824290          9.868765         10.547343          10.529860  \n",
       "12   2.615771          2.456955          2.956695           3.056532  \n",
       "13   6.136140          6.167112          6.710687           6.581566  \n",
       "14  11.396620         11.511399         11.786248          11.637370  \n",
       "15   2.632151          2.375401          2.922296           3.262853  \n",
       "16   5.923493          5.864748          6.353111           6.204867  \n",
       "17  10.885395         10.940802         11.581193          11.552360  \n",
       "18   2.519764          2.031744          2.221516           2.477166  \n",
       "19   5.527250          5.520651          5.719310           5.832009  \n",
       "20  10.315308         10.551416         10.830093          10.856750  \n",
       "21   2.494361          2.184076          2.432251           2.620945  \n",
       "22   5.532605          5.512678          5.969285           5.918731  \n",
       "23  10.468751         10.683931         10.887201          11.002824  \n",
       "24   2.617337          2.266223          2.501507           2.955923  \n",
       "25   5.831188          5.814391          6.148260           6.222434  \n",
       "26  10.881552         11.049647         11.393198          11.644470  "
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_tune_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of rmse:\n",
      " default              6.960066\n",
      "num_boost_round      6.935800\n",
      "lambda               6.946966\n",
      "alpha                6.931164\n",
      "eta                  6.960066\n",
      "subsample            6.960066\n",
      "max_depth            6.649057\n",
      "min_child_weight     6.630256\n",
      "colsample_bytree     6.960066\n",
      "colsample_bylevel    6.931296\n",
      "dtype: float64 \n",
      "\n",
      "Std of rmse: \n",
      " default              3.653274\n",
      "num_boost_round      3.731838\n",
      "lambda               3.502335\n",
      "alpha                3.627439\n",
      "eta                  3.653274\n",
      "subsample            3.653274\n",
      "max_depth            3.485243\n",
      "min_child_weight     3.308986\n",
      "colsample_bytree     3.653274\n",
      "colsample_bylevel    3.419731\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean of rmse:\\n\", xgb_tune_one.mean(), \"\\n\")\n",
    "print(\"Std of rmse: \\n\", xgb_tune_one.std())\n",
    "xgb_tune_one.to_csv(\"xgb_tune_one.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f83266a9950>"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb_tune_one.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result of the three methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_result = pd.DataFrame(columns=['rf_grid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = []\n",
    "idx = rf_grid_result[\"value\"].idxmin()\n",
    "best = rf_grid_result.iloc[idx,:]\n",
    "for i in range(5, 10):\n",
    "    val.append(best[i])\n",
    "val[3] = int(val[3])\n",
    "val\n",
    "num_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-230-6437e7d7d393>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0myvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     rmse = cross_val_score(mod, df_list[i].iloc[:,1:num_col], yvec, \n\u001b[0;32m---> 17\u001b[0;31m                            scoring=\"neg_root_mean_squared_error\", cv=5)\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mmlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mall_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rf_grid'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    404\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                                 error_score=error_score)\n\u001b[0m\u001b[1;32m    407\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             error_score=error_score)\n\u001b[0;32m--> 248\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    390\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m                     n_samples_bootstrap=n_samples_bootstrap)\n\u001b[0;32m--> 392\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    166\u001b[0m                                                         indices=indices)\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1244\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1246\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1247\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    373\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the parameter search space\n",
    "param = {\n",
    "    'n_estimators': 1000, # \"num.trees\": 500 / \"n_estimators\": 100\n",
    "    'criterion': 'mse',\n",
    "    'bootstrap': True, # replace\n",
    "    'max_samples': None, # sample.fraction\n",
    "    'max_features': \"auto\", ### \"mtry\": \"sqrt\" / \"max_features\": \"auto\" ###\n",
    "    'min_samples_leaf': 1 # \"min.node.size\" default: 5 / \"min_samples_leaf\": 1\n",
    "}\n",
    "# Fit the model to each dataset\n",
    "mlist = []\n",
    "mod = RandomForestRegressor(**param)\n",
    "for i in range(num_datasets):\n",
    "    num_col = df_list[i].shape[1]\n",
    "    yvec = df_list[i].iloc[:,0:1].values.ravel()\n",
    "    rmse = cross_val_score(mod, df_list[i].iloc[:,1:num_col], yvec, \n",
    "                           scoring=\"neg_root_mean_squared_error\", cv=5)\n",
    "    mlist.append(abs(rmse).mean())\n",
    "all_result['rf_grid'] = mlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random\n",
    "val = []\n",
    "idx = rf_rs_result[\"value\"].idxmin()\n",
    "best = rf_rs_result.iloc[idx,:]\n",
    "for i in range(5, 10):\n",
    "    val.append(best[i])\n",
    "val[3] = int(val[3])\n",
    "# Define the parameter search space\n",
    "param = {\n",
    "    'n_estimators': val[4], # \"num.trees\": 500 / \"n_estimators\": 100\n",
    "    'criterion': 'mse',\n",
    "    'bootstrap': val[0], # replace\n",
    "    'max_samples': val[2], # sample.fraction\n",
    "    'max_features': val[1], ### \"mtry\": \"sqrt\" / \"max_features\": \"auto\" ###\n",
    "    'min_samples_leaf': val[3] # \"min.node.size\" default: 5 / \"min_samples_leaf\": 1\n",
    "}\n",
    "# Fit the model to each dataset\n",
    "mlist = []\n",
    "mod = RandomForestRegressor(**param)\n",
    "for i in range(num_datasets):\n",
    "    num_col = df_list[i].shape[1]\n",
    "    yvec = df_list[i].iloc[:,0:1].values.ravel()\n",
    "    rmse = cross_val_score(mod, df_list[i].iloc[:,1:num_col], yvec, \n",
    "                           scoring=\"neg_root_mean_squared_error\", cv=5)\n",
    "    mlist.append(abs(rmse).mean())\n",
    "all_result['rf_random'] = mlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bayesian\n",
    "val = []\n",
    "idx = rf_bo_result[\"value\"].idxmin()\n",
    "best = rf_bo_result.iloc[idx,:]\n",
    "for i in range(5, 10):\n",
    "    val.append(best[i])\n",
    "val[3] = int(val[3])\n",
    "# Define the parameter search space\n",
    "param = {\n",
    "    'n_estimators': val[4], # \"num.trees\": 500 / \"n_estimators\": 100\n",
    "    'criterion': 'mse',\n",
    "    'bootstrap': val[0], # replace\n",
    "    'max_samples': val[2], # sample.fraction\n",
    "    'max_features': val[1], ### \"mtry\": \"sqrt\" / \"max_features\": \"auto\" ###\n",
    "    'min_samples_leaf': val[3] # \"min.node.size\" default: 5 / \"min_samples_leaf\": 1\n",
    "}\n",
    "# Fit the model to each dataset\n",
    "mlist = []\n",
    "mod = RandomForestRegressor(**param)\n",
    "for i in range(num_datasets):\n",
    "    num_col = df_list[i].shape[1]\n",
    "    yvec = df_list[i].iloc[:,0:1].values.ravel()\n",
    "    rmse = cross_val_score(mod, df_list[i].iloc[:,1:num_col], yvec, \n",
    "                           scoring=\"neg_root_mean_squared_error\", cv=5)\n",
    "    mlist.append(abs(rmse).mean())\n",
    "all_result['rf_bayesian'] = mlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "# Define the parameter search space\n",
    "dval = {\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"lambda\": 1,\n",
    "    \"alpha\": 0,\n",
    "    \"eta\": 0.3,\n",
    "    \"subsample\": 1,\n",
    "    \"max_depth\": 6,\n",
    "    \"min_child_weight\": 1,\n",
    "    \"colsample_bytree\": 1,\n",
    "    \"colsample_bylevel\": 1\n",
    "}\n",
    "n_boost = 10\n",
    "# Fit the model to each dataset\n",
    "mlist = []\n",
    "for i in range(num_datasets):\n",
    "    bst = xgb.cv(dval, dmat_list[i], num_boost_round=n_boost, nfold=5, metrics='rmse', seed=123, shuffle=True)\n",
    "    mlist.append(bst.iloc[len(bst.index)-1, 2])\n",
    "def_result[\"xgb_rmse\"] = mlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
